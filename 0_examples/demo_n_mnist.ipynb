{
 "cells": [
  {
   "cell_type": "code",
   "id": "bec76f9de6df97aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:53:26.525969Z",
     "start_time": "2024-12-06T12:53:24.334374Z"
    }
   },
   "source": "from data.neuro_mnist_loader import load_n_mnist_batches",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T12:53:26.583937Z",
     "start_time": "2024-12-06T12:53:26.531981Z"
    }
   },
   "cell_type": "code",
   "source": "batches_dataset = load_n_mnist_batches()",
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:53:26.666040Z",
     "start_time": "2024-12-06T12:53:26.661997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = batches_dataset.train_loader\n",
    "print(train_loader)"
   ],
   "id": "5d6fd9070cd67dcd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001EFFF957980>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:56:41.640365Z",
     "start_time": "2024-12-06T12:56:41.424823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for image, label in train_loader:\n",
    "    print(image.shape, label.shape)\n",
    "    break"
   ],
   "id": "fb2d4393aaa8c53d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 150, 2312]) torch.Size([128])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:58:18.682731Z",
     "start_time": "2024-12-06T12:58:18.677343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from utils.globals import device, BATCH_SIZE\n"
   ],
   "id": "b597e5d2336509bd",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:58:20.694222Z",
     "start_time": "2024-12-06T12:58:20.691062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.9"
   ],
   "id": "e96e1c0145135bb0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T13:18:56.616253Z",
     "start_time": "2024-12-06T13:18:56.607977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=12, kernel_size=3, padding=1)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, padding=1)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.fc1 = nn.Linear(in_features=24*8, out_features=10)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states and outputs at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        \n",
    "        cur1 = F.max_pool2d(self.conv1(x), 2)\n",
    "        spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "        print(spk1.shape, mem1.shape)\n",
    "        cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
    "        spk2, mem2 = self.lif2(cur2, mem2)\n",
    "        print(spk2.shape, mem2.shape)\n",
    "\n",
    "        cur3 = self.fc1(spk2.view(BATCH_SIZE, -1))\n",
    "        spk3, mem3 = self.lif3(cur3, mem3)\n",
    "        return spk3, mem3"
   ],
   "id": "5f704f0c85d8c628",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T13:17:58.553762Z",
     "start_time": "2024-12-06T13:17:58.545582Z"
    }
   },
   "cell_type": "code",
   "source": "net = Net().to(device)",
   "id": "b655f83109c260a3",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T13:08:19.730448Z",
     "start_time": "2024-12-06T13:08:19.725198Z"
    }
   },
   "cell_type": "code",
   "source": "net",
   "id": "8200d299af23bb58",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(2, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (lif1): Leaky()\n",
       "  (conv2): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (lif2): Leaky()\n",
       "  (fc1): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  (lif3): Leaky()\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T13:18:00.156973Z",
     "start_time": "2024-12-06T13:17:59.651639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "\n",
    "\n",
    "data = data.to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "data = data.view(data.shape[0], data.shape[1], -1)\n",
    "print(data.shape)\n",
    "data = data[0, 0, :].view(2, 34, 34)\n",
    "print(data.shape)\n",
    "\n",
    "net(data)"
   ],
   "id": "f19e9a99167741e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 150, 2312])\n",
      "torch.Size([2, 34, 34])\n",
      "torch.Size([12, 17, 17]) torch.Size([12, 17, 17])\n",
      "torch.Size([24, 8, 8]) torch.Size([24, 8, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x24 and 192x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[82], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m data \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, :]\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m34\u001B[39m, \u001B[38;5;241m34\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(data\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 12\u001B[0m net(data)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[80], line 28\u001B[0m, in \u001B[0;36mNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     25\u001B[0m spk2, mem2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlif2(cur2, mem2)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(spk2\u001B[38;5;241m.\u001B[39mshape, mem2\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 28\u001B[0m cur3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(spk2\u001B[38;5;241m.\u001B[39mT)\n\u001B[0;32m     29\u001B[0m spk3, mem3 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlif3(cur3, mem3)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m spk3, mem3\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (64x24 and 192x10)"
     ]
    }
   ],
   "execution_count": 82
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
